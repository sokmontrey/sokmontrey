---
layout: "@/layouts/ArticleLayout.astro"
title: Taking Derivative Automatically with Computational Graph
description: At the heart of many machine learning algorithms, there are functions and derivatives. How can a computer program find the derivative of a function automatically? We will explore the concept of computational graph and automatic differentiation. And also, I will include a little design dicisions that I made to create a library in C++ to work with these concept.
type: "article"
tags: ["article", "mathematics", "programming", "machine learning", "graph theory"]
date: "January 23, 2024"
---

import AbstractFunction from './AbstractFunction.svelte';
import FamiliarFunction from './FamiliarFunction.svelte';
import AverageNotInstant from './AverageNotInstant.svelte';
import StraightGraph from './StraightGraph.svelte';
import NotStraightGraph from './NotStraightGraph.svelte';

> <span class='pm-color font-mono'>Note:</span><br />
> This article assumes that you have a basic understanding of functions and derivatives. Below is a quick overview which you can skip if you already know what is the deal with these concepts.

# Overview
## Function

A function tells us how things turn into another. The things that get in/out of a function can be anything, including a function itself.

<AbstractFunction client:load />

This is a very abstract definition of a function. A more familiar form is the one that takes in a number and gives out, potentially, a different number.

$$
f(x) = x^2
$$

<FamiliarFunction client:load />

## Derivative
The concept of derivative is beautiful. I wrote another article purely dedicated to this which you can read [here](/articles/derivative) (probably doesn't exist yet).

All you need to know for this article is that:

1. You want to know how a function $f(x)$ is changing at a specific input $a$.
2. To determine if or how something is changing, we need at least two data points to see the difference.

$$
\frac{f(b) - f(a)}{b - a} 
$$

(divide the change in output by the change in input to find the rate of change)

3. But two is not one. We want to know the rate of change of the function at $a$ not an overall average, from $a$ to $b$.

<AverageNotInstant client:load />

$$
y = f(x) = x^2
$$

4. With derivative, we simply ask: "what if those two points were very close together?" Not on top of one another, but very close. Try moving the red dot closer to the blue one and see how the rate of change match up.

$$
\lim_{b\to a} \frac{f(b) - f(a)}{b - a}
$$

With **Limit** we can make $b$ approach $a$. But remember, $b$ is not equal to $a$, they are very close. This is the derivative of $f(x)$ at $a$. 

Now, we can find the derivative of $f(x)$ at any point $x$. To do so, we increase $x$ by a small amount $h$ then divide the change in output by the change in input.

<div class="p-2 my-4 border border-[var(--pm-color)] border-dashed border-2 ">

$$
\frac{\mathrm{d} f }{\mathrm{d} x} =\; \lim_{h \to 0} \frac{f(x+h) - f(x)} {h}
$$

</div>

This is the **fundamental definition of derivative**. 

5. But we, a sane individual, would love to have a little abstraction. There are derivative rules on **elementary functions** (power, exponential, trig function, etc) that can solve any complex function as long as applied with **combinational derivative rules** (product, chain rule, etc).

<br />

# Why?

Let's make things clear before we continue. It is very easy to represent mathematical expression in a computer program. The function $f(x) = x^2$ can be written in [python](https://www.python.org/) as:
    
```python
def f(x):
    return x**2 # x square
```

And its derivative form with respect to $x$, $\mathrm{d}f/\mathrm{d}x = 2x$ (we will talk about this later), can be written as:

```python 
def df(x):
    return 2*x
```

So, [why](https://www.merriam-webster.com/dictionary/why) am I writing this article? why can't we just... write it? Why do we need **computational graph**? And what is a graph anyway? 

We simply don't want to figure out and write the derivative of any function by hand. It is now a computer's job.

# Computational Graph

Consider the function:

$$
f(x) = e^{5x + 3}
$$

It can look very intimidating. But it is just a series of operations. First we multiply $x$ by $5$, then add $3$, and finally raise $e$ to the result.

Below is how you might visualize the operations:

<StraightGraph client:load />

By the way, this is a graph, a computational graph. It is consisted of connected nodes, each representing a math operation. The edges are the flow of output from one operation to another.

Let's take a look at another function:

$$
{\color{white} g(x) = {\color{orange}\sin}^2{\color{orange}(x)} + 3{\color{orange} \sin(x)} }
$$

And here is its graph:

<NotStraightGraph client:load />

One thing to note here is that when creating computational graph, we need to avoid redundant nodes and operations. The $sin(x)$ is being used twice in the function.

# Automatic Differentiation

This should be enough for us to get started with automatic differentiation. We will get back to the more complex idea later once we have rough image of everything.

There are multiple ways to find derivative of a function. We will only focus on one specific automatic differentiation method. But before that, let's quickly go through the other methods and see why we don't use them.

## Symbolic Differentiation
This is the one we might be familiar with the most. We have a function, we follow the rules of derivative, and we get another function which is the derivative of the original function with respect to one of the input.

For example we have the following function:

$$
f(x) = x^2 + sin(x) 
$$

To find its derivative with repsect to $x$, we start from the top.
1. We have two terms, $x^2$ and $sin(x)$, summed together. Treat them like two separate functions.

<br />

## Numerical Differentiation
## Automatic Differentiation

# Implementation with OOP

{/*
VERSION 2: 
x -> NOTE: this article assume you know what a mathematical function is.
x -> If not, here is a quick overview
-> But why? optimization, derivative, gradient, etc.
-> Can't we just write the function and the derivative?
-> Machine learning application.
-> Beside from the optimization with tensor inputs, we want to find the gradient automatically.
*/}
